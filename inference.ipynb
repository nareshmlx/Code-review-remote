{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ce213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 8739\n",
      "Test examples: 1543\n",
      "\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50378/624688682.py:37: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úì Fast inference enabled (Unsloth)\n",
      "\n",
      "====================================================================================================\n",
      "CODE REVIEW MODEL TESTING - PROPER INFERENCE\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "SAMPLE 1 (Index: 1309)\n",
      "====================================================================================================\n",
      "\n",
      "üìã PR Info:\n",
      "  Title: Image Segmentation .cpp tutorial\n",
      "  File: doc/tutorials/imgproc/imgtrans/distance_transformation/distance_transform.markdown\n",
      "  Language: text\n",
      "\n",
      "üìù Code Change:\n",
      "--------------------------------------------------------------------------------\n",
      "@@ -0,0 +1,165 @@\n",
      "+Image Segmentation with Distance Transform and Watershed Algorithm {#tutorial_distance_transform}\n",
      "+=============\n",
      "+\n",
      "+Goal\n",
      "+----\n",
      "+\n",
      "+In this tutorial you will learn how to:\n",
      "+\n",
      "+-   Use the OpenCV function @ref cv::filter2D in order to perform some laplacian filtering for image sharpening\n",
      "+-   Use the OpenCV function @ref cv::distanceTransform in order to obtain the derived represent...\n",
      "\n",
      "‚úÖ EXPECTED REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "Since this and all following code snippets are taken directly from example file, it is better to use `snippet` doxygen command.\n",
      "You can read about it here: http://docs.opencv.org/ref/master/d4/db1/tutorial_documentation.html#tutorial_documentation_commands_include\n",
      "And more details here:\n",
      "http://www.stack.nl/~dimitri/doxygen/manual/commands.html#cmdsnippet\n",
      "\n",
      "\n",
      "üîÑ Generating review comment...\n",
      "\n",
      "ü§ñ MODEL REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "> The difference between these tutorials is that they do not use CMakeLists.txt file or have .cpp/.h/.hpp files\n",
      "\n",
      "I think we should add this information into the `doc/tutorials/introduction/table_of_content_introductory_tutorial.html` page (and/or other relevant pages)\n",
      "\n",
      "üìä ANALYSIS:\n",
      "  Expected length: 357 chars\n",
      "  Generated length: 270 chars\n",
      "  Quality: ‚úÖ Output looks reasonable\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "SAMPLE 2 (Index: 228)\n",
      "====================================================================================================\n",
      "\n",
      "üìã PR Info:\n",
      "  Title: Workflow to build and test on ARM64 for 4.x branch\n",
      "  File: .github/workflows/PR-4.x-ARM64.yaml\n",
      "  Language: yaml\n",
      "\n",
      "üìù Code Change:\n",
      "--------------------------------------------------------------------------------\n",
      "@@ -0,0 +1,169 @@\n",
      "+name: PR:4.x ARM64\n",
      "+\n",
      "+on:\n",
      "+  pull_request:\n",
      "+    branches:\n",
      "+      - 4.x\n",
      "+\n",
      "+env:\n",
      "+  EXTRA_CMAKE_OPTIONS: '-DBUILD_DOCS=ON -DPYTHON_DEFAULT_EXECUTABLE=/usr/bin/python3 -DBUILD_opencv_xfeatures2d=OFF -DBUILD_EXAMPLES=ON -DOPENCV_ENABLE_NONFREE=ON -DENABLE_CCACHE=OFF'\n",
      "+  OPENCV_TEST_DATA_PATH: '/opencv_extra/testdata'\n",
      "+  OPENCV_DOCKER_WORKDIR: '/__w/opencv/opencv'\n",
      "+  PR_AUTHOR: ${{ g...\n",
      "\n",
      "‚úÖ EXPECTED REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "To be honest I don't remember why they were disabled. @asmorkalov, do you remember?\n",
      "\n",
      "Btw, we can turn them on and check.\n",
      "\n",
      "üîÑ Generating review comment...\n",
      "\n",
      "ü§ñ MODEL REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "It is not possible to build python bindings for `contrib` modules (which are not part of the main repo). This should be fixed here:\n",
      "\n",
      "https://github.com/opencv/opencv/blob/master/.github/workflows/PR-4.x-ARM64.yaml#L52\n",
      "\n",
      "```\n",
      "- DBUILD_opencv_xfeatures2d=OFF\n",
      "```\n",
      "\n",
      "üìä ANALYSIS:\n",
      "  Expected length: 122 chars\n",
      "  Generated length: 264 chars\n",
      "  Quality: ‚úÖ Output looks reasonable\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "SAMPLE 3 (Index: 51)\n",
      "====================================================================================================\n",
      "\n",
      "üìã PR Info:\n",
      "  Title: Fix CMake issues on OpenBSD\n",
      "  File: cmake/OpenCVPCHSupport.cmake\n",
      "  Language: cmake\n",
      "\n",
      "üìù Code Change:\n",
      "--------------------------------------------------------------------------------\n",
      "@@ -16,6 +16,11 @@ IF(CMAKE_COMPILER_IS_GNUCXX)\n",
      " \n",
      "     IF(NOT CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"4.2.0\")\n",
      "         SET(PCHSupport_FOUND TRUE)\n",
      "+        IF(${CMAKE_SYSTEM_NAME} STREQUAL \"OpenBSD\")\n",
      "+            SET(PCHSupport_FOUND FALSE)\n",
      "+        ELSE(${CMAKE_SYSTEM_NAME} STREQUAL \"OpenBSD\")\n",
      "\n",
      "‚úÖ EXPECTED REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "yes, your right. Thanks\n",
      "\n",
      "üîÑ Generating review comment...\n",
      "\n",
      "ü§ñ MODEL REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "Please use `elseif` instead of `else(${CMAKE_SYSTEM_NAME})`.\n",
      "\n",
      "üìä ANALYSIS:\n",
      "  Expected length: 23 chars\n",
      "  Generated length: 60 chars\n",
      "  Quality: ‚úÖ Output looks reasonable\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "SAMPLE 4 (Index: 1518)\n",
      "====================================================================================================\n",
      "\n",
      "üìã PR Info:\n",
      "  Title: Add support for Conv1D on OpenCV backend\n",
      "  File: modules/dnn/perf/perf_convolution1d.cpp\n",
      "  Language: cpp\n",
      "\n",
      "üìù Code Change:\n",
      "--------------------------------------------------------------------------------\n",
      "@@ -0,0 +1,166 @@\n",
      "+// This file is part of OpenCV project.\n",
      "+// It is subject to the license terms in the LICENSE file found in the top-level directory\n",
      "+// of this distribution and at http://opencv.org/license.html.\n",
      "+\n",
      "+#include \"perf_precomp.hpp\"\n",
      "+#include <opencv2/dnn/shape_utils.hpp>\n",
      "+\n",
      "+namespace opencv_test {\n",
      "+\n",
      "+    struct Conv1DParam_t {\n",
      "+        int kernel;\n",
      "+        struct BlobShape { int dims...\n",
      "\n",
      "‚úÖ EXPECTED REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "> Indentation.\n",
      "\n",
      "Sorry, my formatter broke (again)\n",
      "\n",
      "üîÑ Generating review comment...\n",
      "\n",
      "ü§ñ MODEL REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "It would be nice to see what is happening here https://github.com/opencv/opencv/blob/07528a2cdd90873102b0e5711475839335e7541d/modules/dnn/src/layers/cpu_kernels/fast_gemm.hpp#L27-L40\n",
      "\n",
      "This code does not look correct for any other platform than x86.\n",
      "\n",
      "üìä ANALYSIS:\n",
      "  Expected length: 51 chars\n",
      "  Generated length: 250 chars\n",
      "  Quality: ‚úÖ Output looks reasonable\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "SAMPLE 5 (Index: 563)\n",
      "====================================================================================================\n",
      "\n",
      "üìã PR Info:\n",
      "  Title: Several small stabilization fixes\n",
      "  File: cmake/OpenCVFindIPP.cmake\n",
      "  Language: cmake\n",
      "\n",
      "üìù Code Change:\n",
      "--------------------------------------------------------------------------------\n",
      "@@ -34,6 +34,11 @@ unset(IPP_VERSION_MAJOR)\n",
      " unset(IPP_VERSION_MINOR)\n",
      " unset(IPP_VERSION_BUILD)\n",
      " \n",
      "+if (X86 AND UNIX AND NOT APPLE AND NOT ANDROID AND BUILD_SHARED_LIBS)\n",
      "+    message(STATUS \"On 32-bit Linux IPP can not currently be used with dynamic libs. Set BUILD_SHARED_LIBS=OFF\")\n",
      "\n",
      "‚úÖ EXPECTED REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "There is a linker issue: [http://code.opencv.org/issues/4070](http://code.opencv.org/issues/4070)\n",
      "Also some other mentions: \n",
      "- https://software.intel.com/en-us/forums/topic/335961\n",
      "- http://answers.opencv.org/question/37115/opencv-249-make-error/\n",
      "\n",
      "It can be related with the following note from [IPP 8.1 Release Notes](https://software.intel.com/en-us/articles/intel-ipp-81-library-release-notes):\n",
      "\n",
      "> Known Intel¬Æ IPP 8.1 Issues and Limitations:\n",
      "> - On Linux\\* OS, Intel IPP shared libraries for IA-32 contain non-PIC symbols. Each process that uses Intel IPP shared libraries uses a separate copy of the library instead of using one common library.\n",
      "\n",
      "\n",
      "üîÑ Generating review comment...\n",
      "\n",
      "ü§ñ MODEL REVIEW COMMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "> IPP can not currently be used\n",
      "\n",
      "The user might use it or not. The problem is that `BUILD_SHARED_LIBS` affects the way libraries are built in CMake and should be respected. The solution would be to remove the line completely, because `BUILD_SHARED_LIBS` is propagated to subprojects automatically.\n",
      "\n",
      "> Set BUILD_SHARED_LIBS=OFF\n",
      "\n",
      "This is incorrect. Setting the variable doesn't make any sense if you don't plan to build shared libraries. It's better to remove this line as well.\n",
      "\n",
      "üìä ANALYSIS:\n",
      "  Expected length: 649 chars\n",
      "  Generated length: 482 chars\n",
      "  Quality: ‚úÖ Output looks reasonable\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "‚ú® Testing complete!\n",
      "\n",
      "====================================================================================================\n",
      "üí° USAGE TIPS:\n",
      "====================================================================================================\n",
      "\n",
      "For best results when using this model:\n",
      "\n",
      "1. Format input as a chat message with 'user' role\n",
      "2. Use tokenizer.apply_chat_template() with add_generation_prompt=True\n",
      "3. Generate with:\n",
      "   - temperature: 0.7 (balanced)\n",
      "   - top_p: 0.9 (diverse but focused)\n",
      "   - max_new_tokens: 150-200 (reasonable review length)\n",
      "\n",
      "Example:\n",
      "```python\n",
      "messages = [{\"role\": \"user\", \"content\": \"Review this code: ...\"}]\n",
      "inputs = tokenizer.apply_chat_template(\n",
      "    messages, \n",
      "    tokenize=True, \n",
      "    add_generation_prompt=True,\n",
      "    return_tensors=\"pt\"\n",
      ").to(\"cuda\")\n",
      "\n",
      "outputs = model.generate(inputs, max_new_tokens=200, temperature=0.7, top_p=0.9)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"nareshmlx/16k_opencvpr\")\n",
    "\n",
    "if 'train' in dataset and len(dataset) == 1:\n",
    "    split_dataset = dataset['train'].train_test_split(test_size=0.15, seed=42)\n",
    "else:\n",
    "    split_dataset = dataset\n",
    "\n",
    "print(f\"Train examples: {len(split_dataset['train'])}\")\n",
    "print(f\"Test examples: {len(split_dataset['test'])}\")\n",
    "\n",
    "print(\"\\nLoading model and tokenizer...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"nareshmlx/code-reviewer-opencv-16k\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nareshmlx/code-reviewer-opencv-16k\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Enable fast inference (if using unsloth)\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    print(\"‚úì Fast inference enabled (Unsloth)\")\n",
    "except:\n",
    "    print(\"‚úì Standard inference mode\")\n",
    "\n",
    "# Select test samples\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(split_dataset['test'])), 5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CODE REVIEW MODEL TESTING - PROPER INFERENCE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    sample = split_dataset['test'][idx]\n",
    "    \n",
    "    # Get the structured data\n",
    "    instruction = sample.get('instruction', '')\n",
    "    input_data = sample.get('input', '')\n",
    "    expected_output = sample.get('output', '')\n",
    "    \n",
    "    # Parse the JSON input to extract relevant fields\n",
    "    try:\n",
    "        input_dict = json.loads(input_data)\n",
    "        pr_title = input_dict.get('pr_title', 'N/A')\n",
    "        file_path = input_dict.get('file_path', 'N/A')\n",
    "        language = input_dict.get('language', 'N/A')\n",
    "        \n",
    "        # Get the most relevant diff section\n",
    "        target_hunk = input_dict.get('target_hunk', '')\n",
    "        if not target_hunk:\n",
    "            full_diff = input_dict.get('full_diff', '')\n",
    "            # Take first 800 characters of diff for context\n",
    "            target_hunk = full_diff[:800] if full_diff else ''\n",
    "        \n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Sample {i}: Could not parse input JSON, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"SAMPLE {i} (Index: {idx})\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    print(f\"\\nüìã PR Info:\")\n",
    "    print(f\"  Title: {pr_title}\")\n",
    "    print(f\"  File: {file_path}\")\n",
    "    print(f\"  Language: {language}\")\n",
    "    \n",
    "    print(f\"\\nüìù Code Change:\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(target_hunk[:400] + \"...\" if len(target_hunk) > 400 else target_hunk)\n",
    "    \n",
    "    print(f\"\\n‚úÖ EXPECTED REVIEW COMMENT:\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(expected_output)\n",
    "    \n",
    "    # ============================================\n",
    "    # THIS IS THE KEY: Format as proper chat message\n",
    "    # ============================================\n",
    "    \n",
    "    # Create the review request message\n",
    "    user_message = f\"\"\"Review this OpenCV code change:\n",
    "\n",
    "File: {file_path}\n",
    "Language: {language}\n",
    "\n",
    "{target_hunk}\n",
    "\n",
    "Provide a brief, specific code review comment.\"\"\"\n",
    "    \n",
    "    # Format as chat conversation (THIS IS WHAT WORKED IN YOUR NOTEBOOK)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template with generation prompt (CRITICAL!)\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # ‚Üê This tells model to generate response\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    print(f\"\\nüîÑ Generating review comment...\")\n",
    "    \n",
    "    # Generate with parameters that worked in your notebook\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=200,      # Reasonable length for review\n",
    "            temperature=0.7,         # Balanced creativity\n",
    "            top_p=0.9,              # Top-p sampling\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the full output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    # The output will include the user message, so we need to extract the response\n",
    "    if \"assistant\" in generated_text.lower():\n",
    "        # Split on common response markers\n",
    "        parts = generated_text.split(\"assistant\")\n",
    "        if len(parts) > 1:\n",
    "            model_output = parts[-1].strip()\n",
    "            # Clean up any remaining markers\n",
    "            model_output = model_output.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    else:\n",
    "        # Fallback: try to extract text after the user message\n",
    "        user_msg_end = generated_text.rfind(user_message)\n",
    "        if user_msg_end != -1:\n",
    "            model_output = generated_text[user_msg_end + len(user_message):].strip()\n",
    "        else:\n",
    "            model_output = generated_text\n",
    "    \n",
    "    # Clean up output\n",
    "    model_output = model_output.strip()\n",
    "    \n",
    "    print(f\"\\nü§ñ MODEL REVIEW COMMENT:\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(model_output)\n",
    "    \n",
    "    # Simple quality assessment\n",
    "    print(f\"\\nüìä ANALYSIS:\")\n",
    "    print(f\"  Expected length: {len(expected_output)} chars\")\n",
    "    print(f\"  Generated length: {len(model_output)} chars\")\n",
    "    \n",
    "    # Check if output looks reasonable\n",
    "    is_reasonable = (\n",
    "        len(model_output) > 10 and\n",
    "        len(model_output) < 1000 and\n",
    "        not model_output.startswith('{') and\n",
    "        model_output != input_data[:100]\n",
    "    )\n",
    "    \n",
    "    if is_reasonable:\n",
    "        print(f\"  Quality: ‚úÖ Output looks reasonable\")\n",
    "    else:\n",
    "        print(f\"  Quality: ‚ö†Ô∏è  Output may have issues\")\n",
    "    \n",
    "    print(f\"\\n{'='*100}\\n\")\n",
    "\n",
    "print(\"\\n‚ú® Testing complete!\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üí° USAGE TIPS:\")\n",
    "print(\"=\"*100)\n",
    "print(\"\"\"\n",
    "For best results when using this model:\n",
    "\n",
    "1. Format input as a chat message with 'user' role\n",
    "2. Use tokenizer.apply_chat_template() with add_generation_prompt=True\n",
    "3. Generate with:\n",
    "   - temperature: 0.7 (balanced)\n",
    "   - top_p: 0.9 (diverse but focused)\n",
    "   - max_new_tokens: 150-200 (reasonable review length)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "messages = [{\"role\": \"user\", \"content\": \"Review this code: ...\"}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=True, \n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=200, temperature=0.7, top_p=0.9)\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f662f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
